---
- name: Create and configure libvirt VMs
  hosts: libvirt_hosts
  become: true
  gather_facts: true
  
  vars:
    vm_config: "{{ hostvars[target_vm] }}"
    
  tasks:
    - name: Install required system packages
      package:
        name:
          # Libvirt and virtualization
          - libvirt-daemon-system
          - libvirt-clients
          - libvirt-dev
          # QEMU/KVM
          - qemu-kvm
          - qemu-utils
          - qemu-system-x86
          # VM management tools
          - virtinst
          - virt-manager
          - virt-viewer
          - virtiofsd
          # Networking
          - bridge-utils
          - dnsmasq
          # Cloud-init ISO creation
          - genisoimage
          # Python development headers (needed for libvirt-python and lxml)
          - python3-dev
          - gcc
          - pkg-config
          # XML libraries (needed for lxml)
          - libxml2-dev
          - libxslt1-dev
          - zlib1g-dev
        state: present
      tags: setup

    - name: Install Python dependencies for Ansible libvirt modules via uv pip
      command: uv pip install 'libvirt-python>=9.0.0' 'lxml>=4.9.0'
      become: false
      delegate_to: localhost
      tags: setup
      register: uv_pip_result
      changed_when: "'Successfully installed' in uv_pip_result.stdout"

    - name: Ensure libvirtd service is running
      systemd:
        name: libvirtd
        state: started
        enabled: true
      tags: setup

    - name: Add current user to libvirt group
      user:
        name: "{{ ansible_user_id }}"
        groups: libvirt
        append: true
      tags: setup

    - name: Create base images directory
      file:
        path: "{{ base_image_path }}"
        state: directory
        mode: '0755'
      tags: setup

    - name: Create secrets directory
      file:
        path: "../secrets"
        state: directory
        mode: '0755'
      delegate_to: localhost
      become: false
      tags: setup

    - name: Check if SSH key already exists
      stat:
        path: "{{ vm_ssh_key_path }}"
      delegate_to: localhost
      become: false
      register: ssh_key_stat
      tags: setup

    - name: Generate ed25519 SSH key for VMs
      command: >
        ssh-keygen -t ed25519 
        -f {{ vm_ssh_key_path }} 
        -N "" 
        -C "libvirt-vms-key"
      delegate_to: localhost
      become: false
      when: not ssh_key_stat.stat.exists
      tags: setup

    - name: Display SSH key information
      pause:
        echo: false
        seconds: 1
        prompt: |
          {% if ssh_key_stat.stat.exists %}
          ‚úÖ SSH key already exists: {{ vm_ssh_key_path }}
          {% else %}
          ‚úÖ Generated new SSH key: {{ vm_ssh_key_path }}
          {% endif %}
          Public key: {{ vm_ssh_public_key_path }}
      delegate_to: localhost
      become: false
      tags: setup

    - name: Check if static DHCP network needs to be configured
      community.libvirt.virt_net:
        command: info
        name: default
        uri: "{{ libvirt_uri | default('qemu:///system') }}"
      register: network_info
      tags: network

    - name: Setup static DHCP reservations for consistent IP assignments
      block:
        - name: Stop default network
          community.libvirt.virt_net:
            command: destroy
            name: default
            uri: "{{ libvirt_uri | default('qemu:///system') }}"
          ignore_errors: true

        - name: Undefine existing default network
          community.libvirt.virt_net:
            command: undefine
            name: default
            uri: "{{ libvirt_uri | default('qemu:///system') }}"

        - name: Define updated default network with static DHCP reservations
          community.libvirt.virt_net:
            command: define
            name: default
            xml: "{{ lookup('file', 'static-network.xml') }}"
            uri: "{{ libvirt_uri | default('qemu:///system') }}"

        - name: Start default network with static reservations
          community.libvirt.virt_net:
            command: start
            name: default
            uri: "{{ libvirt_uri | default('qemu:///system') }}"

        - name: Set default network to autostart
          community.libvirt.virt_net:
            autostart: true
            name: default
            uri: "{{ libvirt_uri | default('qemu:///system') }}"

        - name: Display static DHCP configuration
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ Static DHCP reservations configured:
              - ubuntu-dev:       192.168.122.10 (MAC: 52:54:00:12:34:10)
              - ubuntu-gpu:       192.168.122.11 (MAC: 52:54:00:12:34:11)
              - DHCP range for other VMs: 192.168.122.100-200
      when: 
        - network_info is defined
      tags: network

    - name: Check if source image exists
      stat:
        path: "{{ source_image_path }}"
      register: source_image_stat
      tags: image

    - name: Copy source image to libvirt images directory
      copy:
        src: "{{ source_image_path }}"
        dest: "{{ base_image_path }}/{{ base_image_name }}"
        mode: '0644'
        remote_src: true
      when: source_image_stat.stat.exists
      tags: image

    - name: Create VM disk from base image
      copy:
        src: "{{ base_image_path }}/{{ base_image_name }}"
        dest: "{{ base_image_path }}/{{ vm_config.vm_name }}.qcow2"
        mode: '0644'
        remote_src: true
      tags: disk

    - name: Get current disk info
      command: qemu-img info --output json {{ base_image_path }}/{{ vm_config.vm_name }}.qcow2
      register: disk_info
      tags: disk

    - name: Parse disk info
      set_fact:
        current_virtual_size: "{{ (disk_info.stdout | from_json)['virtual-size'] }}"
        target_size_bytes: "{{ vm_config.vm_disk_size | default(default_vm_disk_size) | regex_replace('G$', '') | int * 1024 * 1024 * 1024 }}"
      tags: disk

    - name: Resize VM disk (only if target is larger)
      command: >
        qemu-img resize 
        {{ base_image_path }}/{{ vm_config.vm_name }}.qcow2 
        {{ vm_config.vm_disk_size | default(default_vm_disk_size) }}
      when: target_size_bytes | int > current_virtual_size | int
      tags: disk

    - name: Display disk size info
      pause:
        echo: false
        seconds: 1
        prompt: |
          Current virtual size: {{ (current_virtual_size | int / 1024 / 1024 / 1024) | round(1) }}G
          Target size: {{ vm_config.vm_disk_size | default(default_vm_disk_size) }}
          {% if target_size_bytes | int > current_virtual_size | int %}
          ‚úÖ Disk will be resized to {{ vm_config.vm_disk_size | default(default_vm_disk_size) }}
          {% else %}
          ‚ÑπÔ∏è  Disk size unchanged (target not larger than current)
          {% endif %}
      tags: disk

    - name: Create cloud-init config directory for VM
      file:
        path: "/tmp/cloud-init-{{ vm_config.vm_name }}"
        state: directory
        mode: '0755'
      tags: cloud-init

    - name: Read SSH public key content
      slurp:
        src: "{{ vm_ssh_public_key_path }}"
      register: ssh_public_key_content
      delegate_to: localhost
      become: false
      tags: cloud-init

    - name: Set SSH key content as a separate variable
      set_fact:
        vm_ssh_key: "{{ ssh_public_key_content.content | b64decode | trim }}"
      tags: cloud-init

    - name: Generate cloud-init user-data for VM with static IP
      template:
        src: user-data.j2
        dest: "/tmp/cloud-init-{{ vm_config.vm_name }}/user-data"
        mode: '0644'
      tags: cloud-init

    - name: Generate cloud-init meta-data for VM
      template:
        src: meta-data.j2
        dest: "/tmp/cloud-init-{{ vm_config.vm_name }}/meta-data"
        mode: '0644'
      tags: cloud-init

    - name: Create cloud-init ISO for VM
      command: >
        genisoimage -output /var/lib/libvirt/images/{{ vm_config.vm_name }}-cloud-init.iso 
        -volid cidata -joliet -r 
        /tmp/cloud-init-{{ vm_config.vm_name }}/user-data 
        /tmp/cloud-init-{{ vm_config.vm_name }}/meta-data
      tags: cloud-init

    - name: Generate VM XML configuration
      template:
        src: vm-config.xml.j2
        dest: "/tmp/{{ vm_config.vm_name }}.xml"
        mode: '0644'
      tags: xml

    - name: Define VM
      community.libvirt.virt:
        command: define
        xml: "{{ lookup('file', '/tmp/' + vm_config.vm_name + '.xml') }}"
        uri: "{{ libvirt_uri | default('qemu:///system') }}"
      tags: define

    - name: Set VM autostart
      community.libvirt.virt:
        name: "{{ vm_config.vm_name }}"
        autostart: "{{ vm_config.vm_autostart | default(false) }}"
        uri: "{{ libvirt_uri | default('qemu:///system') }}"
      tags: autostart

    - name: Validate GPU passthrough setup (if GPU VM)
      block:
        - name: Check if GPU device is bound to vfio-pci
          shell: |
            if [ -d "/sys/bus/pci/devices/{{ vm_config.gpu_device_id }}/driver" ]; then
              driver=$(basename $(readlink /sys/bus/pci/devices/{{ vm_config.gpu_device_id }}/driver))
              echo "$driver"
            else
              echo "no_driver"
            fi
          register: gpu_driver
          changed_when: false

        - name: Check IOMMU group viability
          shell: |
            iommu_group=$(basename $(readlink /sys/bus/pci/devices/{{ vm_config.gpu_device_id }}/iommu_group))
            echo "IOMMU_GROUP:$iommu_group"
            all_vfio=true
            for device in /sys/kernel/iommu_groups/$iommu_group/devices/*; do
              device_id=$(basename $device)
              if [ -d "$device/driver" ]; then
                driver=$(basename $(readlink $device/driver))
              else
                driver="no_driver"
              fi
              echo "DEVICE:$device_id:$driver"
              if [ "$driver" != "vfio-pci" ]; then
                all_vfio=false
              fi
            done
            if [ "$all_vfio" = true ]; then
              echo "STATUS:OK"
            else
              echo "STATUS:FAIL"
            fi
          register: iommu_group_info
          changed_when: false

        - name: Parse IOMMU group information
          set_fact:
            iommu_group_number: "{{ iommu_group_info.stdout_lines | select('match', '^IOMMU_GROUP:') | first | regex_replace('^IOMMU_GROUP:', '') }}"
            iommu_devices: "{{ iommu_group_info.stdout_lines | select('match', '^DEVICE:') | map('regex_replace', '^DEVICE:', '') | list }}"
            iommu_status: "{{ iommu_group_info.stdout_lines | select('match', '^STATUS:') | first | regex_replace('^STATUS:', '') }}"

        - name: Display IOMMU group information
          pause:
            echo: false
            seconds: 1
            prompt: |
              IOMMU Group {{ iommu_group_number }} contains:
              {% for device_info in iommu_devices %}
              {% set parts = device_info.split(':') %}
              - {{ parts[0] }} -> {{ parts[1] }}
              {% endfor %}

        - name: Fail if IOMMU group is not viable
          fail:
            msg: |
              ‚ùå GPU passthrough is not properly configured!

              GPU Device: {{ vm_config.gpu_device_id }}
              IOMMU Group: {{ iommu_group_number }}

              All devices in IOMMU Group {{ iommu_group_number }} must be bound to vfio-pci:
              {% for device_info in iommu_devices %}
              {% set parts = device_info.split(':') %}
              - {{ parts[0] }}: {{ parts[1] }} {% if parts[1] != 'vfio-pci' %}‚ùå{% else %}‚úÖ{% endif %}
              {% endfor %}

              To fix this issue, you must bind ALL devices in the IOMMU group to vfio-pci:

              {% for device_info in iommu_devices %}
              {% set parts = device_info.split(':') %}
              {% if parts[1] != 'vfio-pci' %}
              # Unbind {{ parts[0] }} from {{ parts[1] }}
              echo "{{ parts[0] }}" | sudo tee /sys/bus/pci/drivers/{{ parts[1] }}/unbind

              # Bind {{ parts[0] }} to vfio-pci
              echo "vfio-pci" | sudo tee /sys/bus/pci/devices/{{ parts[0] }}/driver_override
              echo "{{ parts[0] }}" | sudo tee /sys/bus/pci/drivers/vfio-pci/bind
              {% endif %}
              {% endfor %}

              Or see ansible/NVIDIA-GPU-PASSTHROUGH-GUIDE.md for detailed setup steps.
          when: iommu_status != "OK"
      when: vm_config.gpu_passthrough | default(false) | bool
      tags: [gpu-validation, start]

    - name: Start VM
      community.libvirt.virt:
        name: "{{ vm_config.vm_name }}"
        state: running
        uri: "{{ libvirt_uri | default('qemu:///system') }}"
      tags: start

    - name: Get VM IP address
      community.libvirt.virt:
        command: list_vms
        uri: "{{ libvirt_uri | default('qemu:///system') }}"
      register: vm_list
      tags: info

    - name: Display VM information
      pause:
        echo: false
        seconds: 1
        prompt: |
          VM '{{ vm_config.vm_name }}' has been created and started.
          Connect via VNC: virt-viewer {{ vm_config.vm_name }}
          Or use SSH once the VM is fully booted: ssh {{ vm_ssh_user }}@<VM_IP>
      tags: info

    # Post-deployment setup (NVIDIA, Docker, etc.)
    - name: Prepare VM for post-deployment tasks
      block:
        - name: Wait for VM to be accessible via SSH
          wait_for:
            host: "{{ vm_config.vm_static_ip }}"
            port: 22
            delay: 10
            timeout: 300
            state: started
          delegate_to: localhost
          become: false

        - name: Add VM to in-memory inventory for post-deployment tasks
          add_host:
            name: "{{ vm_config.vm_name }}"
            ansible_host: "{{ vm_config.vm_static_ip }}"
            ansible_user: "{{ vm_ssh_user }}"
            ansible_ssh_private_key_file: "{{ vm_ssh_key_path }}"
            ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
            install_nvidia: "{{ vm_config.install_nvidia | default(false) }}"
            cuda_version: "{{ vm_config.cuda_version | default('13.0') }}"
            install_docker: "{{ vm_config.install_docker | default(false) }}"
            custom_packages: "{{ vm_config.custom_packages | default([]) }}"
            post_install_script: "{{ vm_config.post_install_script | default('') }}"
      when: vm_config.install_nvidia | default(false) | bool or vm_config.install_docker | default(false) | bool or (vm_config.custom_packages | default([]) | length > 0) or (vm_config.post_install_script | default('') | length > 0)
      tags: [post-deploy]

    - name: Display NVIDIA installation starting message
      pause:
        echo: false
        seconds: 1
        prompt: |
          ‚è≥ Starting NVIDIA driver and CUDA {{ vm_config.cuda_version | default('13.0') }} installation...
          This will take several minutes and requires a reboot.
      when: vm_config.install_nvidia | default(false) | bool
      tags: [nvidia, post-deploy]

- name: Post-deployment NVIDIA setup
  hosts: "{{ target_vm | default('') }}"
  become: true
  gather_facts: true
  ignore_unreachable: yes

  tasks:
    - name: Install NVIDIA drivers and CUDA
      block:
        - name: Download CUDA repository keyring
          get_url:
            url: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
            dest: /tmp/cuda-keyring_1.1-1_all.deb
            mode: '0644'

        - name: Install CUDA repository keyring
          apt:
            deb: /tmp/cuda-keyring_1.1-1_all.deb
            state: present

        - name: Update apt cache
          apt:
            update_cache: yes

        - name: Install NVIDIA drivers and CUDA toolkit (silent installation)
          apt:
            name:
              - libnvidia-compute-580
              - nvidia-dkms-580-open
              - cuda-toolkit
              - nvidia-gds
            state: present
            install_recommends: yes
          environment:
            DEBIAN_FRONTEND: noninteractive

        - name: Hold NVIDIA and CUDA packages to prevent auto-updates
          dpkg_selections:
            name: "{{ item }}"
            selection: hold
          loop:
            - libnvidia-compute-580
            - nvidia-dkms-580-open
            - cuda-toolkit
            - nvidia-gds

        - name: Create CUDA environment configuration
          copy:
            dest: /etc/profile.d/cuda.sh
            mode: '0644'
            content: |
              # CUDA environment configuration
              export PATH=${PATH}:/usr/local/cuda-{{ cuda_version | default('13.0') }}/bin
              export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-{{ cuda_version | default('13.0') }}/lib64

        - name: Display pre-reboot message
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ NVIDIA drivers and CUDA toolkit installed successfully
              üîÑ Rebooting VM to load NVIDIA kernel modules...

        - name: Reboot VM to load NVIDIA drivers
          reboot:
            reboot_timeout: 600
            msg: "Rebooting to load NVIDIA drivers"

        - name: Wait for VM to come back online after reboot
          wait_for_connection:
            delay: 10
            timeout: 300

        - name: Verify NVIDIA driver installation
          command: /usr/bin/nvidia-smi
          register: nvidia_smi_output
          changed_when: false
          failed_when: nvidia_smi_output.rc != 0

        - name: Verify CUDA compiler installation
          shell: /usr/local/cuda-{{ cuda_version | default('13.0') }}/bin/nvcc --version
          register: nvcc_output
          changed_when: false
          failed_when: nvcc_output.rc != 0

        - name: Display NVIDIA installation success
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ NVIDIA driver and CUDA installation completed successfully!

              === nvidia-smi output ===
              {{ nvidia_smi_output.stdout }}

              === nvcc version ===
              {{ nvcc_output.stdout }}

              üì¶ Installed packages are locked (held) to prevent auto-updates
              üîß CUDA environment variables configured in /etc/profile.d/cuda.sh

      rescue:
        - name: Display NVIDIA installation failure
          fail:
            msg: |
              ‚ùå NVIDIA driver and CUDA installation failed!

              Please check the VM logs for details:
              ssh {{ ansible_user }}@{{ ansible_host }}
              sudo journalctl -xe

              You can also check DKMS build logs:
              sudo dkms status
      when: install_nvidia | default(false) | bool
      tags: [nvidia, post-deploy]

- name: Post-deployment Docker Engine setup
  hosts: "{{ target_vm | default('') }}"
  become: true
  gather_facts: true
  ignore_unreachable: yes

  tasks:
    - name: Install Docker Engine
      block:
        - name: Install prerequisites for Docker repository
          apt:
            name:
              - ca-certificates
              - curl
            state: present
            update_cache: yes

        - name: Create directory for apt keyrings
          file:
            path: /etc/apt/keyrings
            state: directory
            mode: '0755'

        - name: Download Docker GPG key
          get_url:
            url: https://download.docker.com/linux/ubuntu/gpg
            dest: /etc/apt/keyrings/docker.asc
            mode: '0644'

        - name: Get system architecture
          command: dpkg --print-architecture
          register: system_arch
          changed_when: false

        - name: Get Ubuntu codename
          shell: . /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}"
          register: ubuntu_codename
          changed_when: false

        - name: Add Docker repository to apt sources
          apt_repository:
            repo: "deb [arch={{ system_arch.stdout }} signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu {{ ubuntu_codename.stdout }} stable"
            filename: docker
            state: present

        - name: Update apt cache after adding Docker repository
          apt:
            update_cache: yes

        - name: Install Docker Engine and plugins
          apt:
            name:
              - docker-ce
              - docker-ce-cli
              - containerd.io
              - docker-buildx-plugin
              - docker-compose-plugin
            state: present
            install_recommends: yes
          environment:
            DEBIAN_FRONTEND: noninteractive

        - name: Start and enable Docker service
          systemd:
            name: docker
            state: started
            enabled: yes

        - name: Verify Docker service is running
          systemd:
            name: docker
          register: docker_service_status

        - name: Ensure docker group exists
          group:
            name: docker
            state: present

        - name: Add vagrant user to docker group for non-root access
          user:
            name: vagrant
            groups: docker
            append: yes

        - name: Verify Docker installation with hello-world
          shell: docker run --rm hello-world
          register: docker_hello_output
          changed_when: false

        - name: Display Docker installation success
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ Docker Engine installation completed successfully!

              === Docker Version ===
              {{ docker_hello_output.stdout }}

              === Docker Service Status ===
              Docker service is {{ docker_service_status.status.ActiveState }}

              ‚ÑπÔ∏è  User 'vagrant' has been added to docker group
              ‚ÑπÔ∏è  To use Docker without sudo, log out and back in, or run: newgrp docker

              üîß Docker Compose is available as 'docker compose' (plugin)

      rescue:
        - name: Display Docker installation failure
          fail:
            msg: |
              ‚ùå Docker Engine installation failed!

              Please check the VM logs for details:
              ssh {{ ansible_user }}@{{ ansible_host }}
              sudo journalctl -u docker.service -xe

              Check Docker service status:
              sudo systemctl status docker
      when: install_docker | default(false) | bool
      tags: [docker, post-deploy]

- name: Post-deployment custom package installation
  hosts: "{{ target_vm }}"
  become: true
  gather_facts: true
  ignore_unreachable: yes

  tasks:
    - name: Install custom deb packages from URLs
      block:
        - name: Download custom deb package
          get_url:
            url: "{{ item.url }}"
            dest: "/tmp/{{ item.name }}"
            mode: '0644'
          loop: "{{ custom_packages }}"
          when: custom_packages | default([]) | length > 0

        - name: Install custom deb package
          apt:
            deb: "/tmp/{{ item.name }}"
            state: present
          loop: "{{ custom_packages }}"
          when: custom_packages | default([]) | length > 0

        - name: Clean up downloaded deb packages
          file:
            path: "/tmp/{{ item.name }}"
            state: absent
          loop: "{{ custom_packages }}"
          when: custom_packages | default([]) | length > 0

        - name: Display custom package installation success
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ Custom packages installed successfully!
              {% for pkg in custom_packages %}
              - {{ pkg.name }}
              {% endfor %}
          when: custom_packages | default([]) | length > 0

      rescue:
        - name: Display custom package installation failure
          fail:
            msg: |
              ‚ùå Custom package installation failed!

              Please check the VM for details:
              ssh {{ ansible_user }}@{{ ansible_host }}
      when: custom_packages | default([]) | length > 0
      tags: [custom-packages, post-deploy]

- name: Post-installation script execution
  hosts: "{{ target_vm }}"
  become: true
  gather_facts: true
  ignore_unreachable: yes

  tasks:
    - name: Execute post-installation script
      block:
        - name: Check if post-install script is a file path
          stat:
            path: "{{ post_install_script }}"
          delegate_to: localhost
          become: false
          register: script_file_stat
          when: post_install_script | default('') | length > 0

        - name: Copy local script file to VM
          copy:
            src: "{{ post_install_script }}"
            dest: "/tmp/post-install.sh"
            mode: '0755'
          when:
            - post_install_script | default('') | length > 0
            - script_file_stat.stat.exists | default(false)

        - name: Create inline script on VM
          copy:
            content: "{{ post_install_script }}"
            dest: "/tmp/post-install.sh"
            mode: '0755'
          when:
            - post_install_script | default('') | length > 0
            - not (script_file_stat.stat.exists | default(false))

        - name: Execute post-installation script
          shell: /tmp/post-install.sh
          register: script_output
          when: post_install_script | default('') | length > 0

        - name: Display post-installation script output
          pause:
            echo: false
            seconds: 1
            prompt: |
              ‚úÖ Post-installation script executed successfully!

              === Script Output ===
              {{ script_output.stdout }}

              {% if script_output.stderr %}
              === Script Errors (if any) ===
              {{ script_output.stderr }}
              {% endif %}
          when: post_install_script | default('') | length > 0

        - name: Clean up post-installation script
          file:
            path: "/tmp/post-install.sh"
            state: absent
          when: post_install_script | default('') | length > 0

      rescue:
        - name: Display post-installation script failure
          fail:
            msg: |
              ‚ùå Post-installation script execution failed!

              Script output:
              {{ script_output.stdout | default('') }}

              Script errors:
              {{ script_output.stderr | default('') }}

              Please check the VM for details:
              ssh {{ ansible_user }}@{{ ansible_host }}
      when: post_install_script | default('') | length > 0
      tags: [post-install-script, post-deploy]
